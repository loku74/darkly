Un fichier robots.txt indique aux robots d'exploration d'un moteur de recherche les URL auxquelles il peut accéder sur votre site. Son objectif principal est d'éviter de surcharger votre site de demandes.

On peut y acceder en utilisant l'URL suivante : http://<domain>/robots.txt

En faisant cela on obtient:

User-agent: *
Disallow: /whatever
Disallow: /.hidden

On peut voir deux dossiers non autorisés d'accès par les robots.
Dans /whatever il y a un fichier htpasswd qui contient root:437394baff5aa33daa618be47b75cb49
Routine habituelle, on le décrypte en md5 et on obtient: qwerty123@

En allant sur la pas /admin du site, on se connecte avec les identifiants root:qwerty123@ pour obtenir le flag.
